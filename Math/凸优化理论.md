[toc]

# 基本概念

## 仿射集
##### 仿射集合
`$C \subseteq R^n $`是仿射的 `$\Leftrightarrow$` 对于任意`$x_1,x_2 \in C, \theta \in R$`，有`$\theta x_1 + (1-\theta) x_2 \in C$`。直观上就是过集合内任意两点的直线都在集合内。
##### 仿射组合
如果`$\theta_1 + \theta_2 + \cdots + \theta_k = 1$`， 我们称具有`$\theta_1x_1 + \theta_2x_2 + \cdots + \theta_kx_k $`形式的点为`$x_1,x_2,\cdots, x_k$`的**仿射组合**。

由仿射集合的定义，可以证明一个仿射集合包含其中任意点的仿射组合，即如果`$C$`是一个仿射集合，`$x_1,x_2, \cdot, x_k \in C$`，并且`$\theta_1+\theta_1+\cdots+\theta_k = 1$`， 那么`$\theta_1x_1 + \theta_2x_2 + \cdots + \theta_kx_k \in C$`。

##### 仿射子空间
如果`$C$`是一个仿射集合，并且`$x_0\in C$`，则集合
```math
V = C - x_0 = \{x-x_0|x\in C\} 
```
是一个子空间，即关于加法和数乘是封闭的。

**证明：**

  设`$v_1,v_2 \in V, \alpha,\beta \in R$`，则有`$v_1 + x_0 \in C, v_2+x_0 \in C$`，因为`$C$`是仿射的，且`$\alpha + \beta + (1-\alpha - \beta) = 1$`，所以
  ```math
  \alpha v_1 + \beta v_2 + x_0 = \alpha(v_1+x_0) + \beta(v_2 + x_0) + (1-\alpha-\beta)x_0 \in C
  ```
  由于`$\alpha v_1 + \beta v_2 + x_0 \in C$`，我们可知`$\alpha v_1+\beta v_2 \in V$`
  因此仿射集合`$C$`可以表示为：
  ```math
  C = V+x_0=\{v + x_0|x\in V\}
  ```
  即一个子空间加上一个偏移。

我们定义仿射集合`$C$`的维数为子空间`$V=C-x_0$`的维数，其中，`$x_0$`是`$C$`中的任意元素。

##### 仿射包
由集合`$C \subseteq R^n$`中的点的所有仿射组合组成的集合为`$C$`的仿射包，记为`$\mathrm{aff}\,C$`：
```math
\mathrm{aff}\,C = \{\theta_1x_1+\theta_2x_2+\cdots+\theta_kx_x|x_1, \cdots,x_k \in C, \theta_1+\cdots+\theta_k = 1\}
```
仿射包是包含`$C$`的最小的仿射集合，也就是说如果`$S$`是满足`$C\subseteq S$`的仿射集合，那么`$\mathrm{aff}\,C \subseteq S$`。


##### 仿射维数
集合`$C$`的仿射维数定义为其仿射包的维数。


##### 相对内部
如果集合`$C\subseteq R^n$`的仿射维数小于`$n$`，也就是说此时仿射包`$\mathrm{aff}\,C$`在`$R^n$`中并不存在真正的内点，我们定义集合`$C$`的相对内部为`$\mathrm{aff}\,C$`的内部，记为`$\mathrm{relint}\,C$`，即
```math
\mathrm{relint}\,C = \{x| x\in C, \exists r>0, \left(B(x,r)\bigcap \mathrm{aff}\,C\right) \subseteq C\}
```
其中，`$B(x,r) = \{y\mid \|y-x\|\le r\}$`。

集合`$C$`的相对边界为`$\mathrm{cl}C\smallsetminus  \mathrm{relint}\,C$`，其中`$\mathrm{cl}\,C$`表示`$C$`的闭包。

## 凸集

##### 凸集
如果`$C$`中任意两点的线段仍在`$C$`中，即对于任意的`$x_1, x_2 \in C$`和满足`$0 \leq \theta \leq 1$`的`$\theta$`都有：
```math
\theta x_1 + (1-\theta)x_2 \in C
```
则集合`$C$`被称为凸集。注意凸集与仿射集的区别是，凸集要求每个`$\theta \ge 0$`，直观上就是线段和直线的差别。

##### 凸包
集合`$C$`中所有点的凸组合组成的集合为其凸包，记为`$\mathrm{conv}\,C$`:
```math
\mathrm{conv}\,C = \{\theta_1x_1+ \cdots + \theta_kx_k|x_i\in C, \theta_i \geq 0, i=1,2,\cdots,k, \theta_1+\theta_2+\cdots+\theta_k = 1\}
```
`$C$`的凸包是包含`$C$`的最小凸集。

## 锥
##### 锥和凸锥
如果对于任意`$x\in C$`和`$\theta \ge 0$`都有`$\theta x\in C$`，我们称集合`$C$`是**锥**。   
如果集合`$C$`是锥，并且是凸的，则称`$C$`为**凸锥**，即对任意`$x_1,x_2\in C$`和`$\theta_1,\theta_2 \ge 0$`，都有：
```math
\theta_1x_1 + \theta_2x_2 \in C
```

##### 锥组合
具有`$\theta_1x_1 + \cdots + \theta_kx_k, \theta_1, \cdots, \theta_k \ge 0$`形式的点称为`$x_1, \cdots, x_k$`的**锥组合**。  
集合`$C$`是凸锥的充要条件是它包含其元素的所有锥组合。  

凸锥在几何上可以直观的看做是以 `0` 为顶点的扇形，注意 `0` 属于凸锥。
##### 锥包
集合`$C$`的锥包是`$C$`中元素的所有锥组合的集合，即
```math
\{\theta_1x_1 + \cdots + \theta_kx_k | x_i \in C, \theta_i \ge 0, i=1,\cdots,k \}
```
它是包含`$C$`的最小的凸锥。

#### 重要例子
* 空集`$\emptyset$`，单点集`$\{x_0\}$`，全空间`$R^n$`都是`$R^n$`的仿射子集（自然也是凸集）。
* 任意直线是仿射的，如果直线通过零点，则是子空间，因此也是凸锥。
* 一条线段是凸的，但不是仿射的（除非退化为一个点）。
* 任意子空间是仿射的，凸锥。

## 没取名字
##### Euclid球
`$R^n$`中的空间`Euclid`球具有下面的形式：
```math
B(x_c, r) = \{ x\mid \|x-x_c\|_2 \le r \} = \{x\mid(x-x_c)^T(x-x_c)\le r^2\}
```
另一种表达方式为：
```math
B(x_c, r) = \{x_c + ru \mid \|u\|_2 \le 1\}
```

##### 椭球
椭球具有的形式如下：
```math
\mathcal{E} = \{x\mid (x-x_c)^TP^{-1}(x-x_c)\le 1\}
```
其中，`$P=P^T \succ 0$`，即`$P$`是对称正定矩阵。向量`$x_c\in R^n$`是椭球的中心，矩阵`$P$`决定了椭球从`$x_c$`向各个方向扩展的幅度。`$\mathcal{E}$`的半轴长度由`$\sqrt \lambda_i$`给出，这里`$\lambda_i$`是`$P$`的特征值。

##### 向量不等式
`$u \preceq v$` 表示`$u_i \leq v_i, i=1,2,\cdots,m$`，称为`$R^m$`上的向量不等式。


##### 交集运算是保凸的
如果`$S_1,S_2$`是凸集，那么`$S_1\bigcap S_2$`也是凸集，这个性质可以扩展到无穷多个集合的交：如果对于任意的`$\alpha \in \mathcal{A}$`，`$S_\alpha$`都是凸的，那么`$\bigcap_{\alpha\in  \mathcal{A}}S_\alpha$`也凸的。



##### 仿射函数
函数`$f: R^n \rightarrow R^m$` 是仿射的，如果它是一个线性函数和一个常数，即具有`$f(x)=Ax+b$`的形式，其中`$A\in R^{m\times n}, b\in R^m$`。

假设`$S\subseteq R^n$`是凸的，并且`$f: R^n \rightarrow R^m$`是仿射函数，那么`$S$`在`$f$`下的像：
```math
f(S) = \{f(x)|x\in S\}
```
也是凸的。类似的如果`$f:R^k\to R^n$`是仿射函数，那么`$S$`在`$f$`下的原像也是凸的。
```math
f^{-1}(S) = \{x\mid f(x)\in S\}
```

##### 广义不等式


##### 超平面分离定理
假设`$C$`和`$D$`是两个不相交的凸集，即`$C\bigcap D=\emptyset$`，那么存在`$a\neq 0$`和`$b$`使得对于所有`$x\in C$`，有`$a^Tx \le b$`，对于所有的`$x\in D$`有`$a^T x\ge b$`。换言之，仿射函数`$a^Tx-b$`在`$C$`中非正，而在`$D$`中非负，超平面`$\{x\mid a^Tx=b\}$`称为集合`$C$`和`$D$`的分离超平面。

##### 支撑超平面
设`$C\subseteq R^n$`，而`$x_0$`是其边界上`$\mathrm{bd}\,C$`上的一点，即
```math
x_0 \in \mathrm{C} = \mathrm{cl}\, C\smallsetminus \mathrm{int}\, C
```
如果`$a\neq 0$`，并且对于任意的`$x\in C$`满足`$a^Tx\le a^Tx_0$`，那么称超平面`$\{x\mid a^Tx=a_0^Tx\}$`为集合`$C$`在点`$x_0$`处的支撑超平面。这等于是说点`$x_0$`与集合`$C$`被超平面所分离`$\{x\mid a^Tx=a^Tx_0\}$`。其几何解释是超平面`$\{x\mid a^Tx=a_0^Tx\}$`与`$C$`相切与点`$x_0$`，而且半空间`$\{x\mid a^Tx\le a^Tx_0\}$`包含`$C$`。

一个基本的结论，被称为支撑超平面定理，表明对于任意非空凸集`$C$`和任意`$x_0\in \mathrm{bd}\,C$`，在`$x_0$`处存在支撑超平面。


# 凸函数
##### 凸函数
函数`$f:R^n\rightarrow R$`是凸的，如果`${\rm dom} f$`是凸集，且对于任意的`$x,y \in {\rm dom}f$`和任意的`$0 \leq \theta \leq 1$`，有：
```math
f(\theta x+(1-\theta)y) \leq \theta f(x) + (1-\theta)f(y)
```

##### 凸函数的性质
* 仿射函数既是凸的，又是凹的；反之，如某个函数既是凸函数，又是凹函数，那一定是仿射函数。
* 函数是凸的，当且仅当其在与定义域相交的任何直线上都是凸的。换言之，函数`$f$`是凸的，当且仅当对于任意`$x\in \mathrm{dom}f$`和任意向量`$v$`，函数`$g(t) = f(x+v)$`是凸的（其定义域为`$\{t\mid x+tv \in \mathrm{dom}f\}$`）。这个性质非常重要，因为它容许我们通过将函数限制在直线上来判断其是否为凸函数。
* 凸函数在其定义域相对内部是连续的，它只可能在相对边界上不连续。

##### 凸函数的一阶充要条件
假设`$f$`可微，则函数`$f$`是凸函数的充要条件是`${\rm dom}f$`是凸集且对于任意的`$x,y\in {\rm dom}f$`，下式成立
```math
f(y) \ge f(x) + \nabla f(x)^T (y-x)
```
可见**凸函数的一阶`${\rm Taylor}$`展开是原函数的一个全局下估计。**
由上式也可以看出，如果`$\exists x \in {\rm dom}f, \nabla f(x)=0$`，那么对于所有的`$y\in{\rm dom}f, f(y) \ge f(x)$`，即`$x$`是函数`$f$`的全局最小值点。 

##### 二阶充要条件
假设函数`$f$`二阶可微，对于开集`$\mathrm{dom}f$`内的任意一点，它的Hessian矩阵或者`$\nabla^2f$`存在，则函数`$f$`是凸函数的充要条件是Hessian矩阵是半正定矩阵：即对于所有的`$x\in \mathrm{dom}f$`，有：
```math
\nabla^2f \succeq 0
```
从几何上理解为函数图像在`$x$`点处具有正的曲率。


##### 下水平集
函数`$f: R^n \rightarrow R$`的`$\alpha-$`下水平集为：
```math
C_\alpha = \{x \in \mathrm{dom}f \mid f(x) \le \alpha\}
```
对于任意的`$\alpha$`值，凸函数的下水平集仍然是凸集。

##### Jensen不等式及其扩展
基本不等式：
```math
f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y)
```
此不等式可以扩展到更多点的凸组合：如果函数`$f$`是凸函数，`$x_1,x_2,\cdots,x_k \in \mathrm{dom}f, \theta_1,\cdots, \theta_k \ge 0,$`且`$\theta_1+ \cdots + \theta_k = 1$`，则下式成立：
```math
f(\theta_1x_1 + \theta_2x_2 + \cdots + \theta_kx_k) \le \theta_1f(x_1) + \cdots + \theta_kx_k
```
考虑凸集时，此不等式可以扩展至无穷项和、积分以及期望。例如，如果在`$S \subseteq \mathrm{dom}f$`上`$p(x) \ge 0$`，且`$\int_Sp(x)dx = 1$`，则当相应的积分存在时，下式成立：
```math
f\left(\int_Sxp(x)dx \right) \le \int_Sf(x)p(x)dx
```
如果`$x$`是随机变量，事件`$x \in \mathrm{dom}f$`发生的概率为1，函数`$f$`是凸函数，当相应的期望存在时，我们有：
```math
f(\mathbf{E}x) \le \mathbf{E}f(x)
```

##### 逐点最大和逐点上确界
如果函数`$f_1$`和`$f_2$`均为凸函数，则二者的逐点最大函数`$f$`:
```math
f(x) = \max\{f_1(x), f_2(x)\}
```
其定义域为`$\mathrm{dom}f = \mathrm{dom}f_1\bigcap\mathrm{f_2}$`，仍然是凸函数。

**证明：**  
```math
\begin{aligned}
f\left(\theta x + (1-\theta)y\right) &= \max\{f_1(\theta x+(1-\theta)y), f_2(\theta x+(1-\theta)y)\}  \\
&\le \max\{\theta f_1(x)+(1-\theta)f(y), \theta f_2(x)+(1-\theta)f_2(y)\}  \\
&\le \theta \max\{f_1(x), f_2(x)\} + (1-\theta)\max\{f_1(y), f_2(y)\}  \\
&= \theta f(x) + (1-\theta) f(y)
\end{aligned}
```
从而说明了函数的凸性。同样，如果函数`$f_1,\cdots,f_m$`为凸函数，则它们的逐点最大函数仍然是凸函数。
```math
f(x) = \max\{f_1(x), f_2(x), \cdots, f_m(x)\}
```

同理可证，凸函数的逐点最小值函数是凹函数。


# 凸优化

### 一般优化问题

```math
\begin{aligned}
\mathrm{minimize} \quad &f_0(x)  \\
\mathrm{subject\ to} \quad &f_i(x)  \le 0, \quad i=1,2,\cdots,m  \\
 &\ h_i(x) = 0, \quad i=1,2,\cdots,p
\end{aligned}
```
优化问题的定义域：
```math
\mathcal{D} = \bigcap\limits_{i=0}^m\mathrm{dom}f_i \cap \bigcap\limits_{i=1}^p\mathrm{dom}h_i
```
当点`$x \in \mathcal{D}$`时，`$x$`是可行的，当优化问题至少有一个可行点时，我们称为可行的，否则称为不可行。所有可行点的集合称为可行集或约束集。

问题的最优值定义为：
```math
p^\ast = \inf\{f_0(x)\mid f_i(x)\le 0,i=1,\cdots,m,\ h_i(x)=0, i=1,2,\cdots,p\} 
```

### 凸优化问题
```math
\begin{aligned}
\mathrm{minimize} \quad &f_0(x) \\ 
\mathrm{subject\ to}\quad &f_i(x) \le 0, \quad i=1,\cdots,m  \\
&a_i^Tx = b_i, \quad i=1,\cdots,p
\end{aligned}
```
其中`$f_0,\cdots,f_m$`为凸函数。  
可以看到：目标函数是凸的，不等式约束是凸的，等式约束是仿射的。

### Lagrange对偶问题

对于一般优化问题，假设定义域（可行域）`$\mathcal{D}$`是非空集合，Lagrange对偶的基本思想是在目标函数中考虑约束条件，即添加约束条件的加权和，得到增广的目标函数，定义**Lagrange函数**`$L: R^n \times R^m \times R^p \rightarrow R$`为：
```math
L(x,\lambda, \nu) = f_0(x) + \sum\limits_{i=1}^n\lambda_if_i(x) + \sum\limits_{i=i}^p\nu_ih_i(x)
```
其中定义域为`$\mathrm{dom}L = \mathcal{D}\times R^m \times R^p$`。`$\lambda_i$`是称为第`$i$`个不等式约束的`$f_i(x)\le 0$`对应的**Lagrange乘子**，类似的，`$\nu_i$`被称为第`$i$`个等式约束`$h_i(x)=0$`对应的Lagrange乘子。向量`$\lambda,\nu$`被称为**对偶变量**或者是**Lagrange乘子向量**。

### Lagrange对偶函数
定义**Lagrange对偶函数**`$g: R^m \times R^p \rightarrow R$`为Lagrange函数关于`$x$`取得的最小值：即对`$\lambda \in R^m, \nu\in R^p$`，有
```math
g(\lambda,\nu) = \inf\limits_{x\in\mathcal{D}}L(x,\lambda,\nu) = \inf\limits_{x\in\mathcal{D}}\left(f_0(x)+\sum\limits_{i=1}^m\lambda_if_i(x) + \sum\limits_{i=1}^p\nu_ih_i(x)\right)
```
如果Lagrange函数关于`$x$`无下界，则对偶函数取值为`$-\infty$`。因为对偶函数是一族关于`$(\lambda,\nu)$`的仿射函数的逐点下确界，所以即使原问题不是凸的，对偶函数也是凹的。


#### 最优值的下界

**Lagrange对偶函数**构成的原问题的最优值`$p^\ast$`的下界：即对于任意的`$\lambda \succeq 0$`和`$\nu$`下式成立
```math
g(\lambda, \nu) \le p^\ast
```
**证明：** 
设`$\tilde{x}$`是原问题的一个可行点，即`$f_i(\tilde{x}) \le 0, h_i(\tilde{x})=0$`。根据假设，`$\lambda\succeq 0$`，我们有
```math
\sum\limits_{i=1}^m \lambda_if_i(\tilde{x}) + \sum\limits_{i=1}^p\nu_ih_i(\tilde{x}) \le 0
```
这是有与左边第一项非正，而第二项为0。根据上面不等式，有
```math
L(\tilde{x},\lambda,\nu) = f_0(\tilde{x}) + \sum\limits_{i=1}^m\lambda_if_i(\tilde{x}) + \sum\limits_{i=1}^p\nu_ih_i(\tilde{x}) \le f_0(\tilde{x})
```
因此
```math
g(\lambda, \nu) = \inf\limits_{x\in\mathcal{D}}L(x,\lambda,\nu) \le L(\tilde{x},\lambda,\nu) \le f_0(\tilde{x})
```
由于每一个可行点`$\tilde{x}$`都满足`$g(\lambda,\nu) \le f_0(\tilde{x})$`，问题得证。

当`$g(\lambda,\nu) = -\infty$`时，上述不等式的意义不大。只有当`$\lambda \succeq 0$`且`$(\lambda,\nu) \in \mathrm{dom}g, g(\lambda,\nu) > -\infty$`时，对偶函数才能给出`$p^\ast$`的一个非平凡下界，满足此条件的`$(\lambda,\nu)$`是**对偶可行**的。



### Lagrange对偶问题

```math
\begin{aligned}
\mathrm{maxmize} \quad &g(\lambda, \nu)  \\
\mathrm{subject\ to} \quad &\lambda \succeq 0
\end{aligned}
```
称对偶问题的最优解`$(\lambda^\ast,\nu^\ast)$`为**对偶最优解**或者**最优Lagrange乘子**。

Lagrange对偶问题是一个凸优化问题，这是因为极大化的目标函数是一个凹函数，且约束集合是凸集。因此，

**对偶问题的凸性与原问题是否是凸优化问题无关，只是原问题的凸性会影响到强对偶是否成立**

关于拉格朗日对偶函数一定是凹函数，作如下证明，为了简化证明过程，我们设对偶函数为一元函数`$g(\lambda)$`：
```math
\begin{aligned}
&g(\theta\lambda_1 + (1-\theta)\lambda_2)  \\
=& \min\limits_x\{ L(x, \theta\lambda_1+(1-\theta)\lambda_2) \} \quad (1)  \\
\ge& \min\limits_x\{\theta L(x,\lambda_1) + (1-\theta)L(x,\lambda_2)\} \quad (2)  \\
\ge& \theta\min\limits_xL(x,\lambda_1) + (1-\theta)\min\limits_xL(x,\lambda_2)  \\
=& \theta g(\lambda_1) + (1-\theta)g(\lambda_2)
\end{aligned}
```
注意上式中(1)式到(2)式，是因为首先关于`$x$`取极小值，此时`$x$`已经是一个固定值，因此`$L(x, \lambda)$`是关于`$\lambda$`的仿射函数，仿射函数是凸函数，同时也是凹函数，我们利用了它的凹性。  
因此得证。

实际上，无需上面的证明。从Lagrange函数的形式上看，就是一族关于`$(\lambda,\nu)$`的仿射函数逐点最小值，仿射函数是凸函数，也是凹函数，对凸函数逐点取极小值得到的是凹函数。

### 弱对偶性
Lagrange对偶问题的最优解，我们用`$d^\ast$`表示，根据定义，这是通过Lagrange对偶函数得到的原问题的最优解`$p^\ast$`的最好下界，我们有：
```math
d^\ast \le p^\ast
```
**即使原问题不是凸问题，上述不等式依然成立，这个性质称为弱对偶性**。  

定义差值`$p^\ast - d^\ast$`是原问题的**最优对偶间隙**。

当原问题很难求解时，弱对偶不等式可以给出原问题最优解的一个下界，这是因为对偶问题总是凸问题，很多情况下可以有效的求解`$d^\ast$`。

### 强对偶性和Slater约束准则

若等式`$d^\ast = p^\ast$`成立，即最优对偶间隙为0，那么强对偶成立。这说明从Lanrange函数得到的最优下界是紧的。

对于一般情况下，强对偶性不成立，但是如果原问题是凸问题，即可表述为如下形式：
```math
\begin{aligned}
\mathrm{minimize} \quad &f_0(x)  \\
\mathrm{subject\ to} \quad &f_i(x) \le 0, i=1,2,\cdots,m  \\
&Ax = b
\end{aligned}
```
其中，`$f_0,f_1,f_2,\cdots,f_m$`是凸函数，**强对偶性<font color='red'>通常</font>成立**。

**如果再满足Slater条件，则强对偶必定成立。**  
**Slater条件：存在一点`$x\in \mathrm{relint}\mathcal{D}$`使得下式成立：**
```math
\begin{aligned}
&f_i(x) < 0, i=1,\cdots,m  \\
&Ax = b
\end{aligned}
```
当不等式约束函数`$f_i$`中有一些是仿射函数时，Slater准则可以进一步放宽，如果最前面的`$k$`个约束函数`$f_1,f_2,\cdots, f_k$`是仿射的，那么下列弱化的条件成立，强对偶性成立。该条件为：存在一点`$x\in \mathrm{relint} \mathcal{D}$`使得：
```math
\begin{aligned}
&f_i(x) \le 0, \quad i = 1,2,\cdots, k  \\
&f_i(x) < 0 , \quad i = k+1,\cdots, m  \\
&Ax = b
\end{aligned}
```
换言之，仿射函数不需要严格成立。注意到当所有的约束函数都是仿射函数，且`$\mathrm{dom}f_0$`是开集时，则上式就是可行性条件。（强对偶一定成立？？？哦对，开集必有内点）



### 互补松弛性

假设强对偶性成立，令`$x^\ast$`是原问题的最优解，`$(\lambda^\ast, \nu^\ast)$`是对偶问题的最优解，这表明
```math
\begin{aligned}
f_0(x^\ast) &= g(\lambda^\ast, \nu^\ast)  \\
&= \inf\limits_x\left(f_0(x) + \sum\limits_{i=1}^m\lambda_i^\ast f_i(x) +\sum\limits_{i=1}^p\nu_i^\ast h_i(x) \right)  \\
&\le f_0(x^\ast) + \sum\limits_{i=1}^m\lambda_i^\ast f_i(x^\ast) + \sum\limits_{i=1}^p\nu_i^\ast h_i(x^\ast) \\
&\le f_0(x^\ast)
\end{aligned}
```
第一个等式说明最优对偶间隙为0，第二个等式是对偶函数的定义。第三个不等式是根据Lagrange函数关于`$x$`求下确界小于等于其在`$x=x^\ast$`处的值得来。最后一个不等式的成立是由于`$\lambda_i \ge 0, f_i(x^\ast) \le 0, i=1,2,\cdots,m$`，以及`$h_i(x^\ast)=0,i=1,2,\cdots,p$`。因此上面的两个不等式取等号。

由于上面的等式成立，
```math
\sum\limits_{i=1}^m\lambda_i^\ast f_i(x^\ast) = 0
```
事实上，求和的每一项都非正，
```math
\lambda_i^\ast f_i(x^\ast) = 0, \quad i=1,2,\cdots,m
```
上述条件称为**互补松弛性**：强对偶成立时，
```math
\lambda_i^\ast > 0 \Longrightarrow f_i(x^\ast) = 0
```
等价的有
```math
f_i(x^\ast) < 0 \Longrightarrow \lambda_i^\ast = 0
```

# KKT最优性条件

现在假设函数`$f_0,f_1,\cdot,f_m,h_1,\cdots,h_p$`可微（因此定义域是开集），注意**并没有**假设这些函数是凸函数。

### 非凸问题的KKT条件

**假设强对偶成立**，另外由于`$L(x,\lambda^\ast,\nu^\ast)$`关于`$x$`在`$x^\ast$`处取得最小值，因此函数在`$x^\ast$`处的导数必须为0，因此我们得到KKT条件：
```math
\begin{aligned}
&f_i(x^\ast) \le 0,  \quad i=1,2,\cdots,m  \\
&h_i(x^\ast) = 0, \quad i=1,2,\cdots,p  \\
&\lambda_i^\ast \ge 0,  \quad i=1,2,\cdots,m  \\
&\lambda_i^\ast f_i(x^\ast) = 0, \quad i=1,2,\cdots,m \\
&\nabla f_0(x^\ast) + \sum\limits_{i=1}^m\lambda_i^\ast\nabla f_i(x^\ast) + \sum\limits_{i=1}^p\nu_i^\ast\nabla h_i(x^\ast) = 0
\end{aligned}
```
**总结:** 对于目标函数和约束函数可微的任意优化问题，如果强对偶成立，那么任意对原问题最优解和对偶问题最优解必须满足KKT条件。

### 凸问题的KKT条件
如果目标函数和约束函数可微的任意凸优化问题，那么任意满足KKT条件的点分别是原、对偶问题的最优解，对偶间隙为0。

为了说明这一点，注意到前面两个条件说明了`$x^\ast$`是原问题的可行解，因为`$\lambda_i^\ast \ge 0$`，`$L(x, \lambda^\ast, \nu^\ast)$`是关于`$x$`的凸函数；最后一个KKT条件说明在`$x=x^\ast$`处，Lagrange函数的导数为0。因此`$L(x, \lambda^\ast, \nu^\ast)$`关于`$x$`在`$x^\ast$`处取得最小值。因此，

```math
\begin{aligned}
g(\lambda^\ast, \nu^\ast) &= \inf\limits_x L(x, \lambda^\ast, \nu^\ast)  \\
&= L(x^\ast, \lambda^\ast, \nu^\ast)  \\
&= f_0(x^\ast) + \sum\limits_{i=1}^m\lambda_i^\ast f_i(x^\ast) + \sum\limits_{i=1}^p\nu_i^\ast h_i(x^\ast)  \\
&= f_0(x^\ast)
\end{aligned}
```

这说明原问题的解`$x^\ast$`和对偶问题的解`$(\lambda^\ast, \nu^\ast)$`之间的对偶间隙为0，因此分别是原、对偶问题的最优解。


### 满足Slater条件的凸问题的KKT条件
因此，综合上面两节的结论可得：
1. 对于目标函数和约束函数可微的<font color='blue'>**任意优化问题**</font>，**如果强对偶成立**，那么任意对于原问题最优解和对偶问题最优解必须满足KKT条件。
2. 对于目标函数和约束函数可微的<font color='blue'>**凸优化问题**</font>，那么任意满足KKT条件的点分别是原、对偶问题的最优解，对偶间隙为0。

综合以上两点，可得下面的定理：

**定理：**  如某个凸优化问题具有可微的目标函数和约束函数，且满足Slater条件，那么KKT条件的最优性的充要条件是：Slater条件意味着最优对偶间隙为0，且对偶最优解可以达到，因此`$x$`是原问题的最优解，当且仅当存在`$(\lambda, \nu)$`，二者满足KKT条件。

# 回溯直线搜索
1. 给定`$f$`在`$x\in \mathrm{dom}\,f$`处的下降方向`$\Delta x$`，参数`$\alpha \in (0,0.5),\, \beta\in (0,1)$`
2. 令`$t=1$`  
3. 如果`$f(x+t\Delta x) > f(x) + \alpha t \nabla f(x)^T\Delta x$`，令`$t:=\beta t$`

正如其名，回溯搜索从单位步长开始，按比例逐渐减小，直到满足停止条件`$f(x+t\Delta x) \le f(x)+\alpha t\nabla f(x)^T \Delta x$`。有`$\Delta x$`是下降方向，`$\nabla f(x)^T\Delta x < 0$`，所以只要`$t$`足够小，就一定有：
```math
f(x+\Delta x) \approx f(x) + t\nabla f(x)^T\Delta x < f(x) + \alpha t \nabla f(x)^T\Delta x
```
因此回溯直线搜索会停止。常数`$\alpha$`表示可以接受的`$f$`的减少量占基于线性外推预测的减少量的比值。参数`$\alpha$`的取值一般的0.01到0.3之间。`$\beta$`的取值一般为0.1到0.8，对应的搜索有粗糙到细腻。

# 最速下降法

对`$f(x+v)$`在`$x$`处进行一阶Taylor展开，
```math
f(x+v) \approx \hat{f}(x+v) = f(x) + \nabla f(x)^Tv
```
其中右边第二项`$\nabla f(x)^T v$`是`$f$`在`$x$`处沿方向`$v$`的**线性变化量**（这个名字是我自己取的）。它近似的给出了`$f$`沿小的步径`$v$`会发生的变化。如果线性变化量是负的，步径`$v$`就是下降方向。

令`$\|\cdot\|$`为`$R^n$`上的任意范数，我们定义一个规范化的最速下降方向（相对于范数`$\|\cdot\|$`），如下：
```math
\Delta x_{\mathrm{nst}} = \arg\min\limits_v\{\nabla f(x)^Tv \|v\|=1\}
```
我们强调的“一个”最速下降方向的原因是可能会有多个最优解。  
一个规范化的最速下降方向`$\Delta x_{\mathrm{nsd}}$`是一个能使`$f$`的线性近似下降最多的具有单位范数的步径。

我们同样可以将`$\Delta x_{\mathrm{nsd}}$`定义为：
```math
\Delta x_{\mathrm{nsd}} = \arg\min\limits_v\{\nabla f(x)^Tv \mid \|v\|\le 1\}
```
因此一个规范化的下降方向就是`$\|\cdot\|$`的单位球体中在`$-\nabla f(x)$`的方向上投影最长的方向。实际上也就是得到`$\nabla f(x)$`的对偶范数的`$v$`的反方向，也就是`$\|\nabla f(x)\|_\ast = -\nabla f(x)^T\Delta x_{\mathrm{nsd}}$`

我们也可以将规范化的最速下降方向乘以一个特殊的比例因子，从而考虑下述非规范化的最速下降方向`$\Delta x_{\mathrm{sd}}$`
```math
\Delta x_{\mathrm{sd}} = \|\nabla f(x)\|_\ast\Delta x_{\mathrm{sd}}
```
其中，`$\|\cdot\|_\ast$`表示对偶范数。对于这种最速下降步径，我们有
```math
\nabla f(x)^T\Delta x_{\mathrm{sd}} = \|\nabla f(x)\|_\ast \nabla f(x)^T\Delta x_{\mathrm{sd}} = - \|\nabla f(x)\|_\ast^2
```
### `Euclid`范数意义下的最速下降法
如果我们将`$\|\cdot\|_\ast$`取为`Euclid`范数，可以看出最速下降方向就是负梯度的方向，即`$\Delta x_{\mathrm{sd}} = -\nabla f(x)$`或者`$\Delta x_{\mathrm{nsd}} = -\frac{\nabla f(x)}{\|\nabla f(x)\|_2}$`

### 二次范数下的最速下降法
问题可以描述为，`$P \in S_{++}$`也就是正定矩阵：
```math
\Delta x_{\mathrm{nsd}} = \arg\min\limits_v\{\nabla f(x)^Tv \mid \|v\|_P\le 1\}
```
限制`$v$`的`$P-$`二次范数为`1`，实际上就是一个椭圆：
```math
(P^{1/2}v)^T (P^{1/2}v) = v^TPv = 1
```
因此问题可以写成:
```math
\arg\min\limits_v \nabla f(x)^Tv = \arg\min\limits_v\nabla f(x)^TP^{-1/2}(P^{1/2}v)
```
可以看出，要取得最小值，`$P^{-1/2}\nabla f(x)$`与`$P^{1/2}v$`方向必然相反，因此存在唯一的负数`$\alpha$`使得：
```math
\begin{aligned}
P^{1/2}v &= \alpha P^{-1/2} \nabla f(x)  \\
v &= \alpha P^{-1}\nabla f(x)
\end{aligned}
```
为了求得`$v$`的具体的值，由于`$\|v\|_P = 1$`
```math
\begin{aligned}
\alpha^2 \nabla f(x)^T P^{-1} P P^{-1}\nabla f(x) = 1  \\
\Rightarrow \alpha = - \frac{1}{(\nabla f(x)^TP^{-1}\nabla f(x))^{1/2}} 
\end{aligned}
```
因此，
```math
\Delta x_{\mathrm{nsd}} =  - \frac{P^{-1}\nabla f(x)}{(\nabla f(x)^TP^{-1}\nabla f(x))^{1/2}} 
```
另外，可以得到`$\nabla f(x)$`的对偶函数：
```math
\|\nabla f(x)\|_\ast = \nabla f(x)^T \Delta x_{\mathrm{nsd}} = (\nabla f(x)^TP^{-1}\nabla f(x))^{1/2}
```
因此，
```math
\Delta x_{\mathrm{sd}} = -P^{-1}\nabla f(x)
```

#### 基于坐标变化的解释
对于最速下降方向`$\Delta x_{\mathrm{sd}}$`，我们可以通过坐标变换给出另一种解释：它是对原问题进行某种坐标变换后的梯度下降方向。定义`$\bar{u} = P^{1/2}u$`，因此`$\|\bar{u}\|_P=\|\bar{u}\|_2$`。采用这种坐标变换，原目标函数`$f$`的极小化问题可以等价转换为极小化下式给出的目标函数`$\bar{f}: R^n\to R$`：
```math
\bar{f}(\bar{u}) = f(P^{-1/2}\bar{u}) = f(u)
```
如果我们采用梯度方法优化`$\bar{f}$`，在点`$\bar{x}$`（对应于原问题的点`$x = P^{-1/2}\bar{x}$`）处的直线搜索方向为：
```math
\begin{aligned}
\Delta \bar{x} &= - \nabla_{\bar{x}} \bar f(\bar x) = - \nabla_{\bar{x}}f(P^{-1/2}\bar x) \\
=& -\nabla_{\bar{x}}f(x) = - \nabla_{\bar x}x  \nabla_{x}f(x) \\
=& -P^{-1/2}\nabla_xf(x)
\end{aligned}
```
把`$\Delta \bar x$`变换回去`$\Delta x$`
```math
\Delta x = P^{-1/2}(-P^{-1/2}\nabla f(x)) = -P^{-1}\nabla f(x)
```
换言之，二次范数`$\|\cdot\|_P$`下的最速下降方向可以理解为对原问题进行坐标变换`$\bar x = P^{1/2}x$`后的梯度方向。


### `$\ell_1-$`范数的最速下降方向


# 牛顿法

#### 牛顿步径
对于`$x\in \mathrm{dom}\,f$`，我们称向量：
```math
\Delta x_{\mathrm{nt}} = -\nabla^2f(x)^{-1}\nabla f(x)
```
为`$f$`在`$x$`处的`Newton`步径。由于`$\nabla^2f(x)$`的正定性可知，除非`$\nabla f(x)=0$`，否则就有
```math
\nabla f(x)^T\Delta x_{\mathrm{nt}} = - \nabla f(x) \nabla^2f(x)^{-1} \nabla f(x) < 0
```
因此`Newton`步径是下降方向。

#### 二阶近似的最优解
函数`$f$`在`$x$`处的二阶Taylor近似`$\hat f$`为：
```math
\hat f(x+v) = f(x) + \nabla f^T(x)v + \frac{1}{2}v^T\nabla^2f(x)v
```
这是关于`$v$`的二次凸函数，两边取梯度，并令其为0
```math
\nabla_v\hat f(x+v) = \nabla_x f(x) + \nabla_x^2f(x)v = 0

\Updownarrow 

v = -\nabla^2f(x)^{-1}\nabla f(x)
```
因此在`$v=\Delta x_{\mathrm{nt}}$`处达到极小值。因此，将`$x$`加上`Newton`步径`$\Delta x_{\mathrm{nt}}$`能够使得`$f$`的二阶近似取得极小值。

如果函数`$f$`是二次的，则`$x+\Delta x_{\mathrm{nt}}$`是`$f$`的精确最优解。如果函数`$f$`近似二次，直观上`$x+\Delta x_{\mathrm{nt}}$`是最优解`$x^\ast$`的很好的估计值。既然`$f$`是二次可微的，当`$x$`靠近`$x^\ast$`时，`$f$`的二次模型应该非常准确，由此可知，当`$x$`靠近`$x^\ast$`时，点`$x+\Delta x_{\mathrm{nt}}$`应该是`$x^\ast$`的很好的估计。

#### `Newton`减量

我们将
```math
\lambda(x) = (\nabla f(x)^T \nabla^2f(x)^{-1}\nabla f(x))^{1/2}
```
称为`$x$`处的`Newton`减量。
```math
f(x) - \inf\limits_y\hat f(y) = f(x) - \hat f(x+\Delta x_{\mathrm{nt}}) = \frac{1}{2} \lambda (x)^2

\nabla f(x)^T \Delta x_{\mathrm{nt}} = - \lambda(x)^2

-\lambda(x)^2 = \nabla f(x)^T\Delta x_{\mathrm{nt}} = \frac{d}{dt}f(x+t\Delta x_{\mathrm{nt}})\mid_{t=0}
```

#### `Newton`法算法流程
给定初始点 `$x\in \mathrm{dom}\,f$`，误差阈值`$\epsilon > 0$`。

重复进行：  
  1. 计算牛顿步径和减量
  ```math
  \Delta x_{\mathrm{nt}} := -\nabla^2 f(x)^{-1};\quad \lambda^2 = \nabla f(x)^T\nabla^2f(x)^{-1}\nabla f(x)
  ```
  2. 停止准则：如果`$\lambda^2/2 \le \epsilon$`，退出。
  3. 直线搜索：通过回溯直线搜索 确定步长`$t$`。
  4. 改进：`$x := x+t\Delta x_{\mathrm{nt}}$`


------
下面的内容参考此处：[牛顿法](https://zhuanlan.zhihu.com/p/37588590)

目标函数的二阶泰勒展开
```math
f(x) = f(x_0) + \nabla^T f(x_0)(x-x_0)+ \frac{1}{2}(x-x_0)^T\nabla^2f(x_0)(x-x_0) + o((x-x_0)^2)
```
忽略二次以上的项，并对上式两边同时求梯度，得到函数的导数为：
```math
\nabla f(x) = \nabla f(x_0) + \nabla^2f(x_0)(x-x_0)
```
其中`$\nabla^2f(x_0)$`即为Hessian矩阵，在后面我们改写成H。令函数的梯度为0，则有：
```math
\nabla f(x_0) + \nabla^2f(x_0)(x-x_0) = 0

\Longrightarrow x = x_0 - (\nabla^2f(x_0))^{-1}\nabla f(x_0)
```
如果将梯度向量改写成`$g$`，上面的公式可以改写为：
```math
x = x_0 - H^{-1}g
```
从初始点`$x_0$`，反复计算函数的Hessian矩阵和梯度，然后进行迭代：
```math
x_{k+1} = x_k - H_k^{-1}g_k
```
最终会达到函数的驻点处。其中，`$-H^{-1}g$`称为牛顿方向。迭代终止的条件是梯度的模接近于0或者函数的下降值小于指定值。


#### 推荐参考
这个总结的非常好：[凸优化问题](https://mp.weixin.qq.com/s?__biz=MzU4MjQ3MDkwNA==&mid=2247484439&idx=1&sn=4fa8c71ae9cb777d6e97ebd0dd8672e7&chksm=fdb69980cac110960e08c63061e0719a8dc7945606eeef460404dc2eb21b4f5bdb434fb56f92#rd)


